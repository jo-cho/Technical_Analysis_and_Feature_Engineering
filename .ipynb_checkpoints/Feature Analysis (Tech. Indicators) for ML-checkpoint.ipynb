{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysts\n",
    "\n",
    "- Transforming raw data into informative signals.\n",
    "- These informative signals have some predictive power over financial variables. \n",
    "- Feature analysts are experts in information theory, signal extraction and processing, visualization, labeling, weighting, classifiers, and feature importance techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha factors:\n",
    "- Alpha factors are transformations of market, fundamental, and alternative data that contain\n",
    "predictive signals. Some factors describe fundamental, economy-wide variables such as\n",
    "growth, inflation, volatility, productivity, and demographic risk. Other factors represent\n",
    "investment styles, such as value or growth, and momentum investing that can be traded\n",
    "and are thus priced by the market. There are also factors that explain price movements\n",
    "based on the economics or institutional setting of financial markets, or investor behavior,\n",
    "including known biases of this behavior.\n",
    "- The economic theory behind factors can be rational so that factors have high returns\n",
    "over the long run to compensate for their low returns during bad times. It can also be\n",
    "behavioral, where factor risk premiums result from the possibly biased, or not entirely\n",
    "rational, behavior of agents that is not arbitraged away.\n",
    "- To avoid false discoveries and ensure a factor delivers consistent results, it should have\n",
    "a meaningful economic intuition based on the various established factor categories like\n",
    "momentum, value, volatility, or quality and their rationales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market features\n",
    "\n",
    "Momentum investing\n",
    "\n",
    "- Momentum investing is among the most well-established factor strategies, underpinned by quantitative evidence since Jegadeesh and Titman (1993) for the US equity market. \n",
    "- Momentum factors are designed to go long on assets that have performed well, while going short on assets with poor performance over a certain period. \n",
    "- Such price momentum defies the hypothesis of efficient markets, which states that past price returns alone cannot predict future performance. \n",
    "\n",
    "Behavioral rationale\n",
    "- The behavioral rationale reflects the biases of underreaction (Hong, Lim, and Stein, 2000) and over-reaction (Barberis, Shleifer, and Vishny, 1998) to market news as investors process new information at different speeds. \n",
    "- After an initial under-reaction to news, investors often extrapolate past behavior and create price momentum. \n",
    "- A fear and greed psychology also motivates investors to increase exposure to winning assets and continue selling losing assets (Jegadeesh and Titman, 2011).\n",
    "\n",
    "Fundamental Drivers\n",
    "- Momentum can also have fundamental drivers such as a positive feedback loop between risk assets and the economy. \n",
    "- Economic growth boosts equities, and the resulting wealth effect feeds back into the economy through higher spending, again fueling growth.\n",
    "\n",
    "Market Microstructure effects\n",
    "- Over shorter, intraday horizons, market microstructure effects can also create price\n",
    "momentum as investors implement strategies that mimic their biases. \n",
    "- These strategies create momentum because they imply an advance commitment to sell when an asset underperforms and buy when it\n",
    "outperforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Backtesting is not a research tool. Feature importance is.\"\n",
    "\n",
    "— Marcos Lopez de Prado\n",
    "\n",
    "Advances in Financial Machine Learning (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found what features are important, we can learn more by conducting\n",
    "a number of experiments. \n",
    "\n",
    "- Are these features important all the time, or only in some specific environments? \n",
    "- What triggers a change in importance over time? \n",
    "- Can those regime switches be predicted? \n",
    "- Are those important features also relevant to other related financial instruments? \n",
    "- Are they relevant to other asset classes? What are the most relevant features across all financial instruments? \n",
    "- What is the subset of features with the highest rank correlation across the entire investment universe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With substitution effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A substiution effect\n",
    "- A substitution effect takes place when the estimated importance of one feature is reduced by the presence of other related features. \n",
    "- Substitution effects are the ML analogue of what the statistics and econometrics literature calls “multi-collinearity.” \n",
    "- One way to address linear substitution effects is to apply PCA on the raw features, and then perform the feature importance analysis on the orthogonal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean decrease impurity (MDI)\n",
    " - At each node of each decision tree, the selected feature splits the subset it received in such a way that impurity is decreased. Therefore, we can derive for each decision tree how much of the overall impurity decrease can be assigned to each feature. And given that we have a forest of trees, we can average those values across all estimators and rank the features accordingly.\n",
    "\n",
    " - Masking effects take place when some features are systematically ignored by tree-based classifiers in favor of others. \n",
    "     - In order to avoid them, set max_features=int(1)when using sklearn’s RF class. In this way, only one random feature is considered per level.\n",
    " - Every feature will have some importance, even if they have no predictive power whatsoever.\n",
    " - MDI cannot be generalized to other non-tree based classifiers.\n",
    " - The method does not address substitution effects in the presence of correlated features. \n",
    "     - MDI dilutes the importance of substitute features, because of their interchangeability: The importance of two identical features will be halved, as they are randomly chosen with equal probability.\n",
    " - Strobl et al. [2007] show experimentally that MDI is biased towards some predictor variables. White and Liu [1994] argue that, in case of single decision trees, this bias is due to an unfair advantage given by popular impurity functions toward predictors with a large number of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Decrease Accuracy\n",
    "\n",
    "- First, it fits a classifier; second, it derives its performance OOS according to some performance score (accuracy, negative log-loss, etc.); third, it permutates each column of the features matrix (X), one column at a time, deriving the performance OOS after each column’s permutation.\n",
    " \n",
    "- This method can be applied to any classifier, not only tree-based classifiers.\n",
    "\n",
    "- MDA is not limited to accuracy as the sole performance score.\n",
    "- Like MDI, the procedure is also susceptible to substitution effects in the presence of correlated features. \n",
    "- Unlike MDI, it is possible that MDA concludes that all features are unimportant. That is because MDA is based on OOS performance.\n",
    "- The CV must be purged and embargoed, for the reasons explained in Chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITHOUT SUBSTITUTION EFFECTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitution effects can lead us to discard important features that happen to be redundant. This is not generally a problem in the context of prediction, but it could lead us to wrong conclusions when we are trying to understand, improve, or simplify a\n",
    "model. For this reason, the following single feature importance method can be a good complement to MDI and MDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Feature Importance\n",
    "\n",
    "Single feature importance (SFI) is a cross-section predictive-importance (out-ofsample)\n",
    "method. It computes the OOS performance score of each feature in isolation.\n",
    "\n",
    "- This method can be applied to any classifier, not only tree-based classifiers.\n",
    "- SFI is not limited to accuracy as the sole performance score.\n",
    "- Unlike MDI and MDA, no substitution effects take place, since only one feature is taken into consideration at a time.\n",
    "- Like MDA, it can conclude that all features are unimportant, because performance is evaluated via OOS CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal Features\n",
    "\n",
    "A partial solution is to orthogonalize the features before applying MDI and\n",
    "MDA. An orthogonalization procedure such as principal components analysis (PCA)\n",
    "does not prevent all substitution effects, but at least it should alleviate the impact of\n",
    "linear substitution effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides addressing substitution effects,working with orthogonal features provides\n",
    "two additional benefits: \n",
    "- (1) orthogonalization can also be used to reduce the dimensionality of the features matrix X, by dropping features associated with small eigenvalues. This usually speeds up the convergence of ML algorithms; \n",
    "- (2) the analysis is conducted on features designed to explain the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARALLELIZED VS. STACKED FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least two research approaches to feature importance. \n",
    "\n",
    "1) Parallelized: for each security i in an investment universe i = 1,…, I, we form a dataset (Xi, yi), and derive the feature importance in parallel. Features that are important across a wide variety of instruments are more likely to be associated with an underlying phenomenon, particularly when these feature importances exhibit high rank correlation across the criteria. The\n",
    "main advantage of this approach is that it is computationally fast, as it can be parallelized.\n",
    "A disadvantage is that, due to substitution effects, important features may\n",
    "swap their ranks across instruments, increasing the variance\n",
    "\n",
    "2) Stacked: It consists in stacking all datasets {( ̃ Xi, yi)}i=1,…,I into a single combined dataset (X, y), where ̃ Xi is a transformed instance of Xi (e.g., standardized on a rolling trailing window). The purpose of this transformation is to ensure some distributional homogeneity. \n",
    "\n",
    "- Features stacking presents some advantages: \n",
    "    - (1) The classifier will be fit\n",
    "on a much larger dataset than the one used with the parallelized (first) approach; \n",
    "    - (2) the importance is derived directly, and no weighting scheme is required for combining\n",
    "the results; \n",
    "    - (3) conclusions are more general and less biased by outliers or overfitting;\n",
    "and \n",
    "    - (4) because importance scores are not averaged across instruments, substitution\n",
    "effects do not cause the dampening of those scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "AFML (De Prado) \n",
    "- Chapter 8 Feature Importance\n",
    "\n",
    "\n",
    "MLF-FTTP (Dixon et al.)\n",
    "- Chapter 5. Interpretability\n",
    "- Chapter 8. 6. Autoencoders\n",
    "\n",
    "ML4AT (S. Jansen)\n",
    "- Chapter 4: Financial Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
