{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysts\n",
    "\n",
    "- Transforming raw data into informative signals.\n",
    "- These informative signals have some predictive power over financial variables. \n",
    "- Feature analysts are experts in information theory, signal extraction and processing, visualization, labeling, weighting, classifiers, and feature importance techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha factors:\n",
    "- Alpha factors are transformations of market, fundamental, and alternative data that contain\n",
    "predictive signals. Some factors describe fundamental, economy-wide variables such as\n",
    "growth, inflation, volatility, productivity, and demographic risk. Other factors represent\n",
    "investment styles, such as value or growth, and momentum investing that can be traded\n",
    "and are thus priced by the market. There are also factors that explain price movements\n",
    "based on the economics or institutional setting of financial markets, or investor behavior,\n",
    "including known biases of this behavior.\n",
    "- The economic theory behind factors can be rational so that factors have high returns\n",
    "over the long run to compensate for their low returns during bad times. It can also be\n",
    "behavioral, where factor risk premiums result from the possibly biased, or not entirely\n",
    "rational, behavior of agents that is not arbitraged away.\n",
    "- To avoid false discoveries and ensure a factor delivers consistent results, it should have\n",
    "a meaningful economic intuition based on the various established factor categories like\n",
    "momentum, value, volatility, or quality and their rationales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market features\n",
    "\n",
    "Momentum investing\n",
    "\n",
    "- Momentum investing is among the most well-established factor strategies, underpinned by quantitative evidence since Jegadeesh and Titman (1993) for the US equity market. \n",
    "- Momentum factors are designed to go long on assets that have performed well, while going short on assets with poor performance over a certain period. \n",
    "- Such price momentum defies the hypothesis of efficient markets, which states that past price returns alone cannot predict future performance. \n",
    "\n",
    "Behavioral rationale\n",
    "- The behavioral rationale reflects the biases of underreaction (Hong, Lim, and Stein, 2000) and over-reaction (Barberis, Shleifer, and Vishny, 1998) to market news as investors process new information at different speeds. \n",
    "- After an initial under-reaction to news, investors often extrapolate past behavior and create price momentum. \n",
    "- A fear and greed psychology also motivates investors to increase exposure to winning assets and continue selling losing assets (Jegadeesh and Titman, 2011).\n",
    "\n",
    "Fundamental Drivers\n",
    "- Momentum can also have fundamental drivers such as a positive feedback loop between risk assets and the economy. \n",
    "- Economic growth boosts equities, and the resulting wealth effect feeds back into the economy through higher spending, again fueling growth.\n",
    "\n",
    "Market Microstructure effects\n",
    "- Over shorter, intraday horizons, market microstructure effects can also create price\n",
    "momentum as investors implement strategies that mimic their biases. \n",
    "- These strategies create momentum because they imply an advance commitment to sell when an asset underperforms and buy when it\n",
    "outperforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Projection Models\n",
    "\n",
    "PCA\n",
    "- The objective of PCA is to find linear combinations of the original predictors such that the\n",
    "combinations summarize the maximal amount of variation in the original predictor\n",
    "space. From a statistical perspective, variation is synonymous with information. So,\n",
    "by finding combinations of the original predictors that capture variation, we find the subspace of the data that contains the information relevant to the predictors.\n",
    "- PCA is a particularly useful tool when the available data are composed of one or\n",
    "more clusters of predictors that contain redundant information (e.g., predictors that\n",
    "are highly correlated with one another).\n",
    "\n",
    "Kernel PCA\n",
    "- Principal component analysis is an effective dimension reduction technique when\n",
    "predictors are linearly correlated and when the resulting scores are associated with the\n",
    "response. However, the orthogonal partitioning of the predictor space may not provide\n",
    "a good predictive relationship with the response, especially if the true underlying\n",
    "relationship between the predictors and the response is *non-linear*.\n",
    "- The kernel PCA approach combines\n",
    "a specific mathematical view of PCA with kernel functions and the kernel ‘trick’\n",
    "to enable PCA to expand the dimension of the predictor space in which dimension\n",
    "reduction is performed\n",
    "\n",
    "ICA\n",
    "- Independent component analysis (ICA) is similar to PCA in a number of ways. It creates new components that are linear combinations of the original\n",
    "variables but does so in a way that the components are as statistically independent\n",
    "from one another as possible. This enables ICA to be able to model a broader\n",
    "set of trends than PCA, which focuses on orthogonality and linear relationships.\n",
    "There are a number of ways for ICA to meet the constraint of statistical independence\n",
    "and often the goal is to maximize the “non-Gaussianity” of the resulting\n",
    "components.\n",
    "\n",
    "NMF\n",
    "- Non-negative matrix factorization is another linear projection method that is specific to features that are greater than or equal to zero. In this case, the algorithm finds the coefficients of A such that their values are also non-negative (thus ensuring that the new features have the same property). The method for determining the coefficients is conceptually simple: find the best set\n",
    "of coefficients that make the scores as “close” as possible to the original data with\n",
    "the constraint of non-negativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "- Autoencoders are computationally complex multivariate methods for finding representations\n",
    "of the predictor data and are commonly used in deep learning models\n",
    "- The idea is to create a nonlinear mapping between the\n",
    "original predictor data and a set of artificial features (that is usually the same size).\n",
    "These new features, which may not have any sensible interpretation, are then used as\n",
    "the model predictors. While this does sound very similar to the previous projection\n",
    "methods, autoencoders are very different in terms of how the new features are derived\n",
    "and also in their potential benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN\n",
    "- CNNs filter the data and then exploit data locality, either spatial, temporal, or even spatio-temporal, to efficiently represent the input data. When applied to time series, CNNs are non-linear autoregressive models which can be designed to capture multiple scales in the data using dilated convolution. We might use a CNN autoencoder to compress spatial data.\n",
    "\n",
    "PCA and Deep Autoencoder\n",
    "- Principal component analysis is one of the most powerful techniques for dimension reduction and uses orthogonal projection to decorrelate the features. The first m singular values of the weight matrix in a linear autoencoder are the m loading vectors used as an orthogonal basis for projection.\n",
    "- With non-linear activation, the autoencoder can no longer resolve the loading vectors. However, the addition of a more expressive, non-linear, model is used to reduce the reconstruction error for a given compression dimension m.\n",
    "\n",
    "\n",
    "We can combine these different architectures together to build powerful regressions and compression methods. For example, we might use a GRU-autoencoder to compress non-stationary time series where as we might use a CNN autoencoder to compress spatial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Backtesting is not a research tool. Feature importance is.\"\n",
    "\n",
    "— Marcos Lopez de Prado\n",
    "\n",
    "Advances in Financial Machine Learning (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found what features are important, we can learn more by conducting\n",
    "a number of experiments. \n",
    "\n",
    "- Are these features important all the time, or only in some specific environments? \n",
    "- What triggers a change in importance over time? \n",
    "- Can those regime switches be predicted? \n",
    "- Are those important features also relevant to other related financial instruments? \n",
    "- Are they relevant to other asset classes? What are the most relevant features across all financial instruments? \n",
    "- What is the subset of features with the highest rank correlation across the entire investment universe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With substitution effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A substiution effect\n",
    "- A substitution effect takes place when the estimated importance of one feature is reduced by the presence of other related features. \n",
    "- Substitution effects are the ML analogue of what the statistics and econometrics literature calls “multi-collinearity.” \n",
    "- One way to address linear substitution effects is to apply PCA on the raw features, and then perform the feature importance analysis on the orthogonal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean decrease impurity (MDI)\n",
    " - At each node of each decision tree, the selected feature splits the subset it received in such a way that impurity is decreased. Therefore, we can derive for each decision tree how much of the overall impurity decrease can be assigned to each feature. And given that we have a forest of trees, we can average those values across all estimators and rank the features accordingly.\n",
    "\n",
    " - Masking effects take place when some features are systematically ignored by tree-based classifiers in favor of others. \n",
    "     - In order to avoid them, set max_features=int(1)when using sklearn’s RF class. In this way, only one random feature is considered per level.\n",
    " - Every feature will have some importance, even if they have no predictive power whatsoever.\n",
    " - MDI cannot be generalized to other non-tree based classifiers.\n",
    " - The method does not address substitution effects in the presence of correlated features. \n",
    "     - MDI dilutes the importance of substitute features, because of their interchangeability: The importance of two identical features will be halved, as they are randomly chosen with equal probability.\n",
    " - Strobl et al. [2007] show experimentally that MDI is biased towards some predictor variables. White and Liu [1994] argue that, in case of single decision trees, this bias is due to an unfair advantage given by popular impurity functions toward predictors with a large number of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Decrease Accuracy\n",
    "\n",
    "- First, it fits a classifier; second, it derives its performance OOS according to some performance score (accuracy, negative log-loss, etc.); third, it permutates each column of the features matrix (X), one column at a time, deriving the performance OOS after each column’s permutation.\n",
    " \n",
    "- This method can be applied to any classifier, not only tree-based classifiers.\n",
    "\n",
    "- MDA is not limited to accuracy as the sole performance score.\n",
    "- Like MDI, the procedure is also susceptible to substitution effects in the presence of correlated features. \n",
    "- Unlike MDI, it is possible that MDA concludes that all features are unimportant. That is because MDA is based on OOS performance.\n",
    "- The CV must be purged and embargoed, for the reasons explained in Chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITHOUT SUBSTITUTION EFFECTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitution effects can lead us to discard important features that happen to be redundant. This is not generally a problem in the context of prediction, but it could lead us to wrong conclusions when we are trying to understand, improve, or simplify a\n",
    "model. For this reason, the following single feature importance method can be a good complement to MDI and MDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Feature Importance\n",
    "\n",
    "Single feature importance (SFI) is a cross-section predictive-importance (out-ofsample)\n",
    "method. It computes the OOS performance score of each feature in isolation.\n",
    "\n",
    "- This method can be applied to any classifier, not only tree-based classifiers.\n",
    "- SFI is not limited to accuracy as the sole performance score.\n",
    "- Unlike MDI and MDA, no substitution effects take place, since only one feature is taken into consideration at a time.\n",
    "- Like MDA, it can conclude that all features are unimportant, because performance is evaluated via OOS CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal Features\n",
    "\n",
    "A partial solution is to orthogonalize the features before applying MDI and\n",
    "MDA. An orthogonalization procedure such as principal components analysis (PCA)\n",
    "does not prevent all substitution effects, but at least it should alleviate the impact of\n",
    "linear substitution effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides addressing substitution effects,working with orthogonal features provides\n",
    "two additional benefits: \n",
    "- (1) orthogonalization can also be used to reduce the dimensionality of the features matrix X, by dropping features associated with small eigenvalues. This usually speeds up the convergence of ML algorithms; \n",
    "- (2) the analysis is conducted on features designed to explain the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARALLELIZED VS. STACKED FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least two research approaches to feature importance. \n",
    "\n",
    "1) Parallelized: for each security i in an investment universe i = 1,…, I, we form a dataset (Xi, yi), and derive the feature importance in parallel. Features that are important across a wide variety of instruments are more likely to be associated with an underlying phenomenon, particularly when these feature importances exhibit high rank correlation across the criteria. The\n",
    "main advantage of this approach is that it is computationally fast, as it can be parallelized.\n",
    "A disadvantage is that, due to substitution effects, important features may\n",
    "swap their ranks across instruments, increasing the variance\n",
    "\n",
    "2) Stacked: It consists in stacking all datasets {( ̃ Xi, yi)}i=1,…,I into a single combined dataset (X, y), where ̃ Xi is a transformed instance of Xi (e.g., standardized on a rolling trailing window). The purpose of this transformation is to ensure some distributional homogeneity. \n",
    "\n",
    "- Features stacking presents some advantages: \n",
    "    - (1) The classifier will be fit\n",
    "on a much larger dataset than the one used with the parallelized (first) approach; \n",
    "    - (2) the importance is derived directly, and no weighting scheme is required for combining\n",
    "the results; \n",
    "    - (3) conclusions are more general and less biased by outliers or overfitting;\n",
    "and \n",
    "    - (4) because importance scores are not averaged across instruments, substitution\n",
    "effects do not cause the dampening of those scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Advances in Financial Machine Learning (De Prado) \n",
    "- Chapter 8 Feature Importance\n",
    "\n",
    "\n",
    "Machine Learning in Finance - From Theory to Practice (Dixon et al.)\n",
    "- Chapter 5. Interpretability\n",
    "- Chapter 8-6. Autoencoders\n",
    "\n",
    "Machine Learning for Algorithm Trading (S. Jansen)\n",
    "- Chapter 4: Financial Feature Engineering\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
